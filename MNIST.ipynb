{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import torch.optim\n",
    "import torch.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import *\n",
    "from plotting import *\n",
    "from globals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234\n",
    "torch.manual_seed(random_seed)\n",
    "n_epochs = 50\n",
    "batch_size_train = 20\n",
    "batch_size_test = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])#, transforms.Lambda(lambda x : torch.flatten(x))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data/', train=True, download=True, transform=mnist_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data/', train=False, download=True, transform=mnist_transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CIFAR data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "results_folder = './cifar10_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train(net, trainloader, testloader, name=\"\", n_epochs=4, lr=0.001):\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optim = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    def train(epoch):\n",
    "        net.train()\n",
    "        for batch_idx, (data, target_idx) in enumerate(trainloader):\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            data = data.float().to(DEVICE)\n",
    "            #data = torch.squeeze(data).float().to(DEVICE)\n",
    "            target_idx = target_idx.to(DEVICE)\n",
    "            target = nn.functional.one_hot(target_idx,num_classes=10).float()\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                net.update_backwards()\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            if batch_idx % 1000 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                    100. * batch_idx / len(trainloader), loss.item()))\n",
    "                train_losses.append(loss.item())\n",
    "                train_counter.append(\n",
    "                    (batch_idx*64) + ((epoch-1)*len(trainloader.dataset)))\n",
    "                #return None\n",
    "                \n",
    "    \n",
    "\n",
    "    def test():\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target_idx in testloader:\n",
    "                #data = torch.squeeze(data).float().to(DEVICE)\n",
    "                data=data.float().to(DEVICE)\n",
    "                target_idx = target_idx.to(DEVICE)\n",
    "                target=nn.functional.one_hot(target_idx,num_classes=10).float()\n",
    "                output = net(data)\n",
    "                test_loss += loss_fn(output, target).item()\n",
    "                pred_idx = torch.argmax(output.data, dim=-1)\n",
    "                correct += pred_idx.eq(target_idx.data).sum().item()\n",
    "            test_loss /= len(testloader.dataset)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracy.append(100. * correct / len(testloader.dataset))\n",
    "            print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                    test_loss, correct, len(testloader.dataset),\n",
    "                    100. * correct / len(testloader.dataset)))\n",
    "            \n",
    "    torch.save(net.state_dict(), './results/model_{0}.pth'.format(name))\n",
    "    torch.save(optim.state_dict(), './results/optimizer_{0}.pth'.format(name))\n",
    "            \n",
    "    #test()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    \n",
    "    return {\"train_losses\": train_losses, \"train_counter\": train_counter, \"test_losses\" : test_losses, \"test_accuracy\" : test_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_net = FullyConnected(grad_type='pseudo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = ConvMNIST(grad_type=\"pseudo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.705900\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.115618\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.951878\n",
      "\n",
      "Test set: Avg. loss: 0.0114, Accuracy: 8718/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.746857\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.255119\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.442362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ps_data \u001b[38;5;241m=\u001b[39m \u001b[43mtest_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconvbp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mtest_train\u001b[0;34m(net, trainloader, testloader, name, n_epochs, lr)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#test()\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     test()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_counter\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_counter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m : test_losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m : test_accuracy}\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mtest_train.<locals>.train\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_backwards\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/hessian/models.py:92\u001b[0m, in \u001b[0;36mConvMNIST.update_backwards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m     l\u001b[38;5;241m.\u001b[39mupdate_backwards()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_layers:\n\u001b[0;32m---> 92\u001b[0m     \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_backwards\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/hessian/modules.py:123\u001b[0m, in \u001b[0;36mpseudolinear.update_backwards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_backwards\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    122\u001b[0m     new_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_inv \u001b[38;5;241m=\u001b[39m \u001b[43mget_pinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_W\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m#print(torch.norm(self.W_inv - W_inv, p='fro'))\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/hessian/agfunctions.py:18\u001b[0m, in \u001b[0;36mget_pinv\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     16\u001b[0m A_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(A)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     A_ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     20\u001b[0m     A_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((A\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mshape),dtype\u001b[38;5;241m=\u001b[39mDTYPE,device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ps_data = test_train(convnet, trainloader, testloader, name=\"convbp\", n_epochs=4, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bp_data['test_losses'])\n",
    "#plt.yscale('log')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "z = torch.randn((x.shape), dtype=torch.double, device=DEVICE, requires_grad=True)\n",
    "\n",
    "import torch.autograd.gradcheck\n",
    "\n",
    "net = ConvBP()\n",
    "\n",
    "torch.autograd.gradcheck(net, z, eps=1e-4, atol=1e-4, nondet_tol=1.0, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = next(iter(trainloader))\n",
    "x = x.float().to(DEVICE)\n",
    "y = bp_convnet(x)\n",
    "print(torch.argmax(y,dim=-1))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_data = test_train(ps_net, trainloader, testloader, name=\"pseudo\")\n",
    "torch.save(ps_data, './results/ps_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bp_data['test_losses'])\n",
    "plt.plot(ps_data['test_losses'])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bp_data['test_accuracy'])\n",
    "plt.plot(ps_data['test_accuracy'])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = testset[0]\n",
    "\n",
    "output = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=0)\n",
    "out_softmax = softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((10,4,3,3))\n",
    "b = torch.Tensor([2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b[None,...,None,None] * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c != c).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu-only",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
